{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Count with MapReduce - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Now that we have seen the key map and reduce operators in Spark, and also know when to use transformation and action operators, we can revisit the word count problem we introduced earlier in the section. In this lab, we will read a text corpus into the Spark environment, perform a word count, and try basic NLP ideas to get a good grip on how MapReduce performs. \n",
    "\n",
    "## Objectives\n",
    "\n",
    "In this lab you will:\n",
    "\n",
    "- Apply the map(func) transformation to a given function on all elements of an RDD in different partitions \n",
    "- Apply a map transformation for all elements of an RDD \n",
    "- Compare the difference between a transformation and an action within RDDs \n",
    "- Use collect(), count(), and take() actions to trigger spark transformations \n",
    "- Use filter to select data that meets certain specifications within an RDD \n",
    "- Use Spark and the MapReduce framework to complete a full parallelized word count problem \n",
    "\n",
    "## MapReduce task\n",
    "\n",
    "Here is what our problem looks like:\n",
    "\n",
    "* We have a huge text document\n",
    "* We need to count the number of times each distinct word appears in the document\n",
    "\n",
    "\n",
    "* Sample applications:\n",
    "\n",
    "    * Analyze web server logs to find popular URLs\n",
    "    * Analyze texts for content or style "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Count\n",
    "\n",
    "We will illustrate a MapReduce computation for counting the number of occurrences for each word in a text corpus. In this example, the input file is a repository of documents and each document is an element. We shall count the frequency of stop words for __style identification__ as stop words might have unique features which can potentially describe the author's writing style based on their use of stop words while writing. We shall look at some texts by Shakespeare and Jane Austin following this motivation. \n",
    "\n",
    "MapReduce in PySpark provides a practical and efficient way of achieving this goal as it: \n",
    "\n",
    "* works if the file is too large for memory\n",
    "\n",
    "* works even if the output is too large for memory\n",
    "\n",
    "* is naturally parallelizable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MapReduce Framework\n",
    "\n",
    "Here are the steps that we will perform for our problem, under the MapReduce framework:  \n",
    "\n",
    "* Sequentially read a lot of data (text files in this case)\n",
    "\n",
    "\n",
    "* Map:\n",
    "    * Extract something you care about\n",
    "\n",
    "\n",
    "* Group by key: Sort and shuffle\n",
    "\n",
    "\n",
    "* Reduce:\n",
    "    * Aggregate, summarize, filter or transform\n",
    "\n",
    "\n",
    "* Write the result \n",
    "\n",
    "As a reminder, here is what it looks like visually, given the example we used before:\n",
    "![](./images/word_count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize SparkContext()\n",
    "\n",
    "- First, import the `pyspark` module into this Python environment and initialize a `SparkContext()` \n",
    "- Initialize a local spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-47ff3b9c5156>:3 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-47ff3b9c5156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start a local SparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local[*]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-1-47ff3b9c5156>:3 "
     ]
    }
   ],
   "source": [
    "# Start a local SparkContext\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext('local[*]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our code, start with a single text file, `'hamlet.txt'`. First, set a file path variable `file` to the location of `'text/hamlet.txt'`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a path variable for data \n",
    "file = 'text/hamlet.txt'\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Split text file contents into RDD - `sc.textFile()`\n",
    "\n",
    "Previously we used parallelization to read an RDD from a Python list. Here we shall read the text file into Spark RDDs by using `sc.textFile()` method for loading the text file into the `lines` RDD. The documentation on RDDs can be found [here!!](https://spark.apache.org/docs/latest/rdd-programming-guide.html)\n",
    "\n",
    "The `textFile(path)` method reads a text file from the HDFS/local file system/any Hadoop supported file system, into the number of partitions specified and returns it as an RDD of strings. In order to view the contents of the RDD, we will use the `RDD.collect()` method since calling the RDD by name will not return the contents, only the object type and relevant information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file into an RDD using sc.textFile()\n",
    "lines = sc.textFile(file)\n",
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text file has been written in a \"line-by-line\" manner into the RDD. We can access any given entry using simple indexing. \n",
    "\n",
    "- Print a few sample lines from the `lines` RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can also print the whole document, lines by line. \n",
    "\n",
    "- Print complete Hamlet from the lines RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the text, line-by-line\n",
    "# This will output the whole of hamlet text, one line at a time. \n",
    "lines.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that the complete text file is in a `lines` RDD, we can easily use the map function to break it down further into individual words and parallelize it accordingly. \n",
    "\n",
    "__Note: Parallelization is handled by the Spark environment according to available infrastructure and doesn't need any further configuration__.\n",
    "\n",
    "## The MAP function `map(func)`\n",
    "\n",
    "The Map function for this example uses keys that are of type string (the words) and values that are integers. The Map task reads a document and breaks it into its sequence of words `w1, w2, . . . , wn`. It then makes a sequence of key-value pairs for each word where the word itself is the key and the value is always 1. That is, the output of the Map task for this document is the sequence of key-value pairs as shown below:\n",
    "\n",
    "> `(w1, 1), (w2, 1), . . . ,(wn, 1)`\n",
    "\n",
    "This step performs the following two sub-steps:\n",
    "\n",
    "* A splitting step that takes the input dataset from the source and divides it into smaller subsets \n",
    "* A mapping step that takes those smaller subsets and performs an action or computation on each subset \n",
    "\n",
    "\n",
    "### Spark Mapping functions\n",
    "\n",
    "Previously, we saw that:\n",
    "\n",
    "- `map(func)` returns a new distributed dataset formed by passing each element of the source through a function `func`.\n",
    "\n",
    "- `flatMap(func)` maps each input item to 0 or more output items (so `func` should return a seq rather than a single item).\n",
    "\n",
    "`flatMap()` breaks the output of a lambda function into individual RDD elements (as opposed to map).\n",
    "\n",
    "---\n",
    "\n",
    "* Use `RDD.flatMap()` to split the lines by whitespace and collect into one flat RDD.\n",
    "\n",
    "* The transformation is defined in the lambda expression, where the input `x` is defined as producing the result of `x.split(' ')`.\n",
    "\n",
    "* Use the `RDD.take(n)` method to pick `n` words from the top of the sequence. \n",
    "\n",
    "* Use `flatMap()` to break the output of the lambda function into individual RDD elements. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the lines into words based on blanks ' ' and show ten elements from the top \n",
    "\n",
    "\n",
    "words = lines.flatMap(lambda x: x.split(' '))\n",
    "words.take(10)\n",
    "\n",
    "# ['', '1604', '', '', 'THE', 'TRAGEDY', 'OF', 'HAMLET,', 'PRINCE', 'OF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create a Tuple as (k,v)\n",
    "\n",
    "- Map each word to a tuple of (word, 1).\n",
    "\n",
    "Map doesn't break up the output of the lambda expression, meaning that the tuples stay intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a lambda function with map to add a 1 to each word and output a tuple\n",
    "# (word, 1) - Take ten elements\n",
    "\n",
    "word_counts = words.map(lambda x: (x, 1))\n",
    "\n",
    "word_counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the words to lower case to ensure integrity\n",
    "\n",
    "As we can see from the output above, the text contains words in capital as well as lower case. By default, 'THE' and 'the' would be considered two separate words due to case sensitivity. \n",
    "\n",
    "- Modify the map function above to change all the words to lowercase using a `.lower()` inside the lambda function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the words in words tuples to lowercase - take 10 elements \n",
    "word_counts_lower = word_counts.map(lambda x: (x[0].lower(), x[1]) )\n",
    "\n",
    "word_counts_lower.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REDUCE Function\n",
    "The Reduce function’s argument is a pair consisting of a key and its list of associated values as the pairs created above. The output of the Reduce function is a sequence of zero or more key-value pairs. These key-value pairs can be of a type different from those sent from Map tasks to Reduce tasks, but often they are the same type.\n",
    "\n",
    "We will refer to the application of the Reduce function to a single key and its associated list of values as a reducer.\n",
    "\n",
    "![](reduce.png)\n",
    "\n",
    "- Use `RDD.reduceByKey()` to add up all the words. the new (k, v) pairs would have the word as a key and the number of occurrences as a value. \n",
    "\n",
    "Here, the lambda has two arguments (x and y) that are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use reduceByKey with tuplesLCase to add all values under same keys - take 10\n",
    "\n",
    "\n",
    "word_counts_lower_reduced = word_counts_lower.reduceByKey(lambda x, y: x + y)\n",
    "word_counts_lower_reduced.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rare words\n",
    "\n",
    "Following the standard NLP approach, we can add a filtering step to remove all words which appear less than some threshold value, say, with less than 5 occurrences. \n",
    "\n",
    "This can be useful to identify common topics between documents, where very rare words can be misleading. \n",
    "For this step, we can use `RDD.filter(func)` where `func` is a lambda function that filters out any word which appears less than or equal to 5 times. You may also use a separate function to achieve this. \n",
    "\n",
    "- Remove rare words with occurences < 5 using lambda function inside a `.filter()` method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 20383),\n",
       " ('the', 1083),\n",
       " ('of', 670),\n",
       " ('hamlet,', 28),\n",
       " ('denmark', 10),\n",
       " ('by', 111),\n",
       " ('king', 43),\n",
       " ('denmark.', 7),\n",
       " ('son', 11),\n",
       " ('to', 727)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all rare words with frequency less than 5 - take 10 \n",
    "no_rare_words = word_counts_lower_reduced.filter(lambda x: x[1] > 5)\n",
    "\n",
    "no_rare_words.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List  of stop words\n",
    "\n",
    "Add a filtering step to retain only words included in a list of stop words. \n",
    "\n",
    "Stop words can be useful for recognizing the style of an author. Removing stop words can be useful in recognizing the topic of a document. For stop word removal, we use `RDD.filter(func)` again with a lambda function that uses a list of stop words to extract the key-value pairs for only the words that are present in the stop word list. Use a simple list like the one shown below: \n",
    "\n",
    "> ['', 'the','a','in','of','on','at','for','by','I','you','me'] \n",
    "\n",
    "\n",
    "- Use the stop word list above to count the occurrences of these words in the document\n",
    "- Show the stop word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 20383),\n",
       " ('the', 1083),\n",
       " ('of', 670),\n",
       " ('by', 111),\n",
       " ('a', 540),\n",
       " ('at', 87),\n",
       " ('you', 433),\n",
       " ('for', 231),\n",
       " ('in', 420),\n",
       " ('me', 144)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show stop word frequency in the output\n",
    "stop_word_list = ['', 'the','a','in','of','on','at','for','by','I','you','me']\n",
    "\n",
    "stop_words = no_rare_words.filter(lambda x: x[0] in stop_word_list)\n",
    "\n",
    "stop_words.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of keep words\n",
    "\n",
    "- Modify the filter operation above to keep all the words found in the text **except** the stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hamlet,', 28),\n",
       " ('denmark', 10),\n",
       " ('king', 43),\n",
       " ('denmark.', 7),\n",
       " ('son', 11),\n",
       " ('to', 727),\n",
       " ('and', 939),\n",
       " ('king.', 119),\n",
       " ('polonius,', 6),\n",
       " ('lord', 16)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify above filter to show top ten keep words by frequency\n",
    "keep_words = no_rare_words.filter(lambda x: x[0] not in stop_word_list)\n",
    "\n",
    "keep_words.take(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together \n",
    "\n",
    "Combine the above code as a function and pass three works of Shakespeare (`'romeandjuliet.txt'`, `'hamlet.txt'`, `'othello.txt'`). Observe the frequency of stop words. Repeat the same exercise for three works of Jane Austin (`'senseandsensibility.txt'`, `'prideandprejudice.txt'` and `'emma.txt'`). \n",
    "\n",
    "> Can you recognize the writing styles of these authors based on their use of stop words?\n",
    "> What can you do to improve the style recognition ability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for word count that takes in a file name and stop wordlist to perform above tasks\n",
    "def wordCount(filename, stopWordlist):\n",
    "    file_path = 'text/'\n",
    "    lines = sc.textFile(file_path+filename)\n",
    "    words = lines.flatMap(lambda x: x.split(' '))\n",
    "    word_counts = words.map(lambda x: (x, 1))\n",
    "    word_counts_lower = word_counts.map(lambda x: (x[0].lower(), x[1]))\n",
    "    word_counts_lower_reduced = word_counts_lower.reduceByKey(lambda x, y: x + y)\n",
    "    no_rare_words = word_counts_lower_reduced.filter(lambda x: x[1] > 5)\n",
    "    stop_words = no_rare_words.filter(lambda x: x[0] in stopWordlist)\n",
    "    return stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 468),\n",
       " ('', 3341),\n",
       " ('the', 767),\n",
       " ('for', 245),\n",
       " ('a', 516),\n",
       " ('at', 76),\n",
       " ('in', 329),\n",
       " ('you', 309),\n",
       " ('on', 84),\n",
       " ('by', 132)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romeoandjuliet = wordCount('romeoandjuliet.txt', stop_word_list)\n",
    "romeoandjuliet.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 20383),\n",
       " ('the', 1083),\n",
       " ('of', 670),\n",
       " ('by', 111),\n",
       " ('a', 540),\n",
       " ('at', 87),\n",
       " ('you', 433),\n",
       " ('for', 231),\n",
       " ('in', 420),\n",
       " ('me', 144)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet = wordCount('hamlet.txt', stop_word_list)\n",
    "hamlet.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 4229),\n",
       " ('of', 538),\n",
       " ('the', 812),\n",
       " ('for', 255),\n",
       " ('a', 472),\n",
       " ('at', 75),\n",
       " ('in', 326),\n",
       " ('you', 449),\n",
       " ('on', 105),\n",
       " ('by', 124)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "othello = wordCount('othello.txt', stop_word_list)\n",
    "othello.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4405),\n",
       " ('of', 3790),\n",
       " ('by', 773),\n",
       " ('', 3652),\n",
       " ('for', 1235),\n",
       " ('at', 844),\n",
       " ('you', 949),\n",
       " ('on', 675),\n",
       " ('in', 2037),\n",
       " ('a', 2141)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sense = wordCount('senseandsensibility 2.txt', stop_word_list)\n",
    "sense.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 4490),\n",
       " ('by', 648),\n",
       " ('', 6539),\n",
       " ('for', 1035),\n",
       " ('of', 3703),\n",
       " ('at', 785),\n",
       " ('you', 1099),\n",
       " ('in', 1895),\n",
       " ('a', 1971),\n",
       " ('on', 685)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prideandprejudice = wordCount('prideandprejudice.txt', stop_word_list)\n",
    "prideandprejudice.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5269),\n",
       " ('of', 4312),\n",
       " ('by', 574),\n",
       " ('', 3322),\n",
       " ('for', 1267),\n",
       " ('at', 1020),\n",
       " ('you', 1588),\n",
       " ('a', 3098),\n",
       " ('in', 2139),\n",
       " ('on', 632)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma = wordCount('emma.txt', stop_word_list)\n",
    "emma.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 5269),\n",
       " ('of', 4312),\n",
       " ('', 3322),\n",
       " ('a', 3098),\n",
       " ('in', 2139),\n",
       " ('you', 1588),\n",
       " ('for', 1267),\n",
       " ('at', 1020),\n",
       " ('on', 632),\n",
       " ('by', 574),\n",
       " ('me', 314)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma.top(50, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([314.0,\n",
       "  809.5,\n",
       "  1305.0,\n",
       "  1800.5,\n",
       "  2296.0,\n",
       "  2791.5,\n",
       "  3287.0,\n",
       "  3782.5,\n",
       "  4278.0,\n",
       "  4773.5,\n",
       "  5269],\n",
       " [3, 2, 1, 1, 0, 1, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emma.map(lambda x: x[1]).histogram(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up (Optional)\n",
    "\n",
    "* Create histograms of the top 50 words from each author\n",
    "\n",
    "\n",
    "## Summary \n",
    "\n",
    "In this simple exercise, we saw MapReduce in action for solving a basic NLP task, i.e. counting the frequency of stop words and keep words of a text corpus. This exercise can be seen as a first step towards text analytics on big data platforms. After this lab, we will get into more advanced use cases of PySpark, specifically for machine learning applications. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
